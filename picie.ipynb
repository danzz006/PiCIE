{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2022-08-20T13:52:15.950925Z","iopub.status.busy":"2022-08-20T13:52:15.950131Z","iopub.status.idle":"2022-08-20T13:53:15.482199Z","shell.execute_reply":"2022-08-20T13:53:15.480997Z","shell.execute_reply.started":"2022-08-20T13:52:15.950864Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["# !pip install torch==1.2.0+gpu torchvision==0.4.0+gpu -f https://download.pytorch.org/whl/torch_stable.html\n","\n","# !conda install pytorch==1.8.1 torchvision==0.9.1 torchaudio==0.8.1 cudatoolkit=11.3 -c pytorch -c conda-forge\n","# !pip install scipy==1.4.1\n","# !pip install faiss-gpu\n","# !pip install scikit-learn==0.22.1\n","# !pip install \"Pillow<7\""]},{"cell_type":"markdown","metadata":{},"source":["## Get dataset"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-08-20T13:04:21.916284Z","iopub.status.busy":"2022-08-20T13:04:21.915891Z","iopub.status.idle":"2022-08-20T13:07:35.106980Z","shell.execute_reply":"2022-08-20T13:07:35.105314Z","shell.execute_reply.started":"2022-08-20T13:04:21.916250Z"},"trusted":true},"outputs":[],"source":["# !wget http://images.cocodataset.org/zips/train2014.zip\n","# !wget http://images.cocodataset.org/zips/val2014.zip"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-08-20T13:09:04.353731Z","iopub.status.busy":"2022-08-20T13:09:04.353084Z","iopub.status.idle":"2022-08-20T13:09:05.335619Z","shell.execute_reply":"2022-08-20T13:09:05.334473Z","shell.execute_reply.started":"2022-08-20T13:09:04.353691Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["assets\t    logs\t\t\t     __pycache__   tools\n","commons.py  modules\t\t\t     querys.npy    train_mdc.py\n","data\t    picie_histogram_coco.pkl\t     README.md\t   train_picie.py\n","env.yml     picie.ipynb\t\t\t     results\t   utils.py\n","K_test\t    picie_retrieval_result_coco.pkl  retrieval.py  visualize.ipynb\n","LICENSE     prepare_histogram.py\t     sh_files\n"]}],"source":["!ls"]},{"cell_type":"code","execution_count":4,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2022-08-20T13:09:08.547037Z","iopub.status.busy":"2022-08-20T13:09:08.546320Z","iopub.status.idle":"2022-08-20T13:10:30.692050Z","shell.execute_reply":"2022-08-20T13:10:30.690443Z","shell.execute_reply.started":"2022-08-20T13:09:08.546995Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["# !unzip train2014.zip\n","# !unzip val2014.zip"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-08-20T13:12:01.341324Z","iopub.status.busy":"2022-08-20T13:12:01.340950Z","iopub.status.idle":"2022-08-20T13:12:02.323358Z","shell.execute_reply":"2022-08-20T13:12:02.322060Z","shell.execute_reply.started":"2022-08-20T13:12:01.341290Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["mkdir: cannot create directory ‘results’: File exists\n"]}],"source":["!mkdir results"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-08-20T11:39:25.726255Z","iopub.status.busy":"2022-08-20T11:39:25.725782Z","iopub.status.idle":"2022-08-20T11:39:26.727678Z","shell.execute_reply":"2022-08-20T11:39:26.726546Z","shell.execute_reply.started":"2022-08-20T11:39:25.726198Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["assets\t    logs\t\t\t     __pycache__   tools\n","commons.py  modules\t\t\t     querys.npy    train_mdc.py\n","data\t    picie_histogram_coco.pkl\t     README.md\t   train_picie.py\n","env.yml     picie.ipynb\t\t\t     results\t   utils.py\n","K_test\t    picie_retrieval_result_coco.pkl  retrieval.py  visualize.ipynb\n","LICENSE     prepare_histogram.py\t     sh_files\n"]}],"source":["!ls"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-08-20T14:59:37.976695Z","iopub.status.busy":"2022-08-20T14:59:37.976319Z","iopub.status.idle":"2022-08-20T14:59:38.041423Z","shell.execute_reply":"2022-08-20T14:59:38.039901Z","shell.execute_reply.started":"2022-08-20T14:59:37.976662Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/nova/anaconda3/envs/picie/lib/python3.8/site-packages/sklearn/utils/linear_assignment_.py:18: FutureWarning: The linear_assignment_ module is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n","  warnings.warn(\n"]}],"source":["import argparse\n","import os\n","import time as t\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from utils import *\n","from commons import * \n","from modules import fpn "]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-08-20T10:43:10.186777Z","iopub.status.busy":"2022-08-20T10:43:10.186422Z","iopub.status.idle":"2022-08-20T10:43:10.203297Z","shell.execute_reply":"2022-08-20T10:43:10.202398Z","shell.execute_reply.started":"2022-08-20T10:43:10.186748Z"},"trusted":true},"outputs":[],"source":["def train(args, logger, dataloader, model, classifier1, classifier2, criterion1, criterion2, optimizer, epoch):\n","    losses = AverageMeter()\n","    losses_mse = AverageMeter()\n","    losses_cet = AverageMeter()\n","    losses_cet_across = AverageMeter()\n","    losses_cet_within = AverageMeter()\n","\n","    # switch to train mode\n","    model.train()\n","    if args.mse:\n","        criterion_mse = torch.nn.MSELoss().cuda()\n","\n","    classifier1.eval()\n","    classifier2.eval()\n","    for i, (indice, input1, input2, label1, label2) in enumerate(dataloader):\n","        input1 = eqv_transform_if_needed(args, dataloader, indice, input1.cuda(non_blocking=True))\n","        label1 = label1.cuda(non_blocking=True)\n","        featmap1 = model(input1)\n","        \n","        input2 = input2.cuda(non_blocking=True)\n","        label2 = label2.cuda(non_blocking=True)\n","        featmap2 = eqv_transform_if_needed(args, dataloader, indice, model(input2))\n","\n","        B, C, _ = featmap1.size()[:3]\n","        if i == 0:\n","            logger.info('Batch input size   : {}'.format(list(input1.shape)))\n","            logger.info('Batch label size   : {}'.format(list(label1.shape)))\n","            logger.info('Batch feature size : {}\\n'.format(list(featmap1.shape)))\n","        \n","        if args.metric_train == 'cosine':\n","            featmap1 = F.normalize(featmap1, dim=1, p=2)\n","            featmap2 = F.normalize(featmap2, dim=1, p=2)\n","\n","        featmap12_processed, label12_processed = featmap1, label2.flatten()\n","        featmap21_processed, label21_processed = featmap2, label1.flatten()\n","\n","        # Cross-view loss\n","        output12 = feature_flatten(classifier2(featmap12_processed)) # NOTE: classifier2 is coupled with label2\n","        output21 = feature_flatten(classifier1(featmap21_processed)) # NOTE: classifier1 is coupled with label1\n","        \n","        loss12  = criterion2(output12, label12_processed)\n","        loss21  = criterion1(output21, label21_processed)  \n","\n","        loss_across = (loss12 + loss21) / 2.\n","        losses_cet_across.update(loss_across.item(), B)\n","\n","        featmap11_processed, label11_processed = featmap1, label1.flatten()\n","        featmap22_processed, label22_processed = featmap2, label2.flatten()\n","        \n","        # Within-view loss\n","        output11 = feature_flatten(classifier1(featmap11_processed)) # NOTE: classifier1 is coupled with label1\n","        output22 = feature_flatten(classifier2(featmap22_processed)) # NOTE: classifier2 is coupled with label2\n","\n","        loss11 = criterion1(output11, label11_processed)\n","        loss22 = criterion2(output22, label22_processed)\n","\n","        loss_within = (loss11 + loss22) / 2. \n","        losses_cet_within.update(loss_within.item(), B)\n","        loss = (loss_across + loss_within) / 2.\n","        \n","        losses_cet.update(loss.item(), B)\n","        \n","        if args.mse:\n","            loss_mse = criterion_mse(featmap1, featmap2)\n","            losses_mse.update(loss_mse.item(), B)\n","\n","            loss = (loss + loss_mse) / 2. \n","        \n","        # record loss\n","        losses.update(loss.item(), B)\n","\n","        # compute gradient and do step\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if (i % 200) == 0:\n","            logger.info('{0} / {1}\\t'.format(i, len(dataloader)))\n","\n","    return losses.avg, losses_cet.avg, losses_cet_within.avg, losses_cet_across.avg, losses_mse.avg"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-08-20T10:43:11.541782Z","iopub.status.busy":"2022-08-20T10:43:11.541287Z","iopub.status.idle":"2022-08-20T10:43:11.572163Z","shell.execute_reply":"2022-08-20T10:43:11.571237Z","shell.execute_reply.started":"2022-08-20T10:43:11.541749Z"},"trusted":true},"outputs":[],"source":["def main(args, logger):\n","    logger.info(args)\n","\n","    # Use random seed.\n","    fix_seed_for_reproducability(args.seed)\n","\n","    # Start time.\n","    t_start = t.time()\n","\n","    # Get model and optimizer.\n","    model, optimizer, classifier1 = get_model_and_optimizer(args, logger)\n","\n","    # New trainset inside for-loop.\n","    inv_list, eqv_list = get_transform_params(args)\n","    trainset = get_dataset(args, mode='train', inv_list=inv_list, eqv_list=eqv_list)\n","    trainloader = torch.utils.data.DataLoader(trainset, \n","                                                batch_size=args.batch_size_cluster,\n","                                                shuffle=False, \n","                                                num_workers=args.num_workers,\n","                                                pin_memory=True,\n","                                                collate_fn=collate_train,\n","                                                worker_init_fn=worker_init_fn(args.seed))\n","    \n","    testset    = get_dataset(args, mode='train_val')\n","    testloader = torch.utils.data.DataLoader(testset,\n","                                             batch_size=args.batch_size_test,\n","                                             shuffle=False,\n","                                             num_workers=args.num_workers,\n","                                             pin_memory=True,\n","                                             collate_fn=collate_eval,\n","                                             worker_init_fn=worker_init_fn(args.seed))\n","    \n","    # Before train.\n","    _, _ = evaluate(args, logger, testloader, classifier1, model)\n","    \n","    if not args.eval_only:\n","        # Train start.\n","        for epoch in range(args.start_epoch, args.num_epoch):\n","            # Assign probs. \n","            trainloader.dataset.mode = 'compute'\n","            trainloader.dataset.reshuffle()\n","\n","            # Adjust lr if needed. \n","            # adjust_learning_rate(optimizer, epoch, args)\n","\n","            logger.info('\\n============================= [Epoch {}] =============================\\n'.format(epoch))\n","            logger.info('Start computing centroids.')\n","            t1 = t.time()\n","            centroids1, kmloss1 = run_mini_batch_kmeans(args, logger, trainloader, model, view=1)\n","            centroids2, kmloss2 = run_mini_batch_kmeans(args, logger, trainloader, model, view=2)\n","            logger.info('-Centroids ready. [Loss: {:.5f}| {:.5f}/ Time: {}]\\n'.format(kmloss1, kmloss2, get_datetime(int(t.time())-int(t1))))\n","            \n","            # Compute cluster assignment. \n","            t2 = t.time()\n","            weight1 = compute_labels(args, logger, trainloader, model, centroids1, view=1)\n","            weight2 = compute_labels(args, logger, trainloader, model, centroids2, view=2)\n","            logger.info('-Cluster labels ready. [{}]\\n'.format(get_datetime(int(t.time())-int(t2)))) \n","            \n","            # Criterion.\n","            if not args.no_balance:\n","                criterion1 = torch.nn.CrossEntropyLoss(weight=weight1).cuda()\n","                criterion2 = torch.nn.CrossEntropyLoss(weight=weight2).cuda()\n","            else:\n","                criterion1 = torch.nn.CrossEntropyLoss().cuda()\n","                criterion2 = torch.nn.CrossEntropyLoss().cuda()\n","\n","            # Setup nonparametric classifier.\n","            classifier1 = initialize_classifier(args)\n","            classifier2 = initialize_classifier(args)\n","            classifier1.module.weight.data = centroids1.unsqueeze(-1).unsqueeze(-1)\n","            classifier2.module.weight.data = centroids2.unsqueeze(-1).unsqueeze(-1)\n","            freeze_all(classifier1)\n","            freeze_all(classifier2)\n","\n","            # Delete since no longer needed. \n","            del centroids1 \n","            del centroids2\n","\n","            # Set-up train loader.\n","            trainset.mode  = 'train'\n","            trainloader_loop  = torch.utils.data.DataLoader(trainset, \n","                                                            batch_size=args.batch_size_train, \n","                                                            shuffle=True,\n","                                                            num_workers=args.num_workers,\n","                                                            pin_memory=True,\n","                                                            collate_fn=collate_train,\n","                                                            worker_init_fn=worker_init_fn(args.seed))\n","\n","            logger.info('Start training ...')\n","            train_loss, train_cet, cet_within, cet_across, train_mse = train(args, logger, trainloader_loop, model, classifier1, classifier2, criterion1, criterion2, optimizer, epoch) \n","            acc1, res1 = evaluate(args, logger, testloader, classifier1, model)\n","            acc2, res2 = evaluate(args, logger, testloader, classifier2, model)\n","            \n","            logger.info('============== Epoch [{}] =============='.format(epoch))\n","            logger.info('  Time: [{}]'.format(get_datetime(int(t.time())-int(t1))))\n","            logger.info('  K-Means loss   : {:.5f} | {:.5f}'.format(kmloss1, kmloss2))\n","            logger.info('  Training Total Loss  : {:.5f}'.format(train_loss))\n","            logger.info('  Training CE Loss (Total | Within | Across) : {:.5f} | {:.5f} | {:.5f}'.format(train_cet, cet_within, cet_across))\n","            logger.info('  Training MSE Loss (Total) : {:.5f}'.format(train_mse))\n","            logger.info('  [View 1] ACC: {:.4f} | mIoU: {:.4f}'.format(acc1, res1['mean_iou']))\n","            logger.info('  [View 2] ACC: {:.4f} | mIoU: {:.4f}'.format(acc2, res2['mean_iou']))\n","            logger.info('========================================\\n')\n","            \n","\n","            torch.save({'epoch': epoch+1, \n","                        'args' : args,\n","                        'state_dict': model.state_dict(),\n","                        'classifier1_state_dict' : classifier1.state_dict(),\n","                        'classifier2_state_dict' : classifier2.state_dict(),\n","                        'optimizer' : optimizer.state_dict(),\n","                        },\n","                        os.path.join(args.save_model_path, 'checkpoint_{}.pth.tar'.format(epoch)))\n","            \n","            torch.save({'epoch': epoch+1, \n","                        'args' : args,\n","                        'state_dict': model.state_dict(),\n","                        'classifier1_state_dict' : classifier1.state_dict(),\n","                        'classifier2_state_dict' : classifier2.state_dict(),\n","                        'optimizer' : optimizer.state_dict(),\n","                        },\n","                        os.path.join(args.save_model_path, 'checkpoint.pth.tar'))\n","        \n","        # Evaluate.\n","        trainset    = get_dataset(args, mode='eval_val')\n","        trainloader = torch.utils.data.DataLoader(trainset, \n","                                                    batch_size=args.batch_size_cluster,\n","                                                    shuffle=True,\n","                                                    num_workers=args.num_workers,\n","                                                    pin_memory=True,\n","                                                    collate_fn=collate_train,\n","                                                    worker_init_fn=worker_init_fn(args.seed))\n","\n","        testset    = get_dataset(args, mode='eval_test')\n","        testloader = torch.utils.data.DataLoader(testset, \n","                                                batch_size=args.batch_size_test,\n","                                                shuffle=False,\n","                                                num_workers=args.num_workers,\n","                                                pin_memory=True,\n","                                                collate_fn=collate_eval,\n","                                                worker_init_fn=worker_init_fn(args.seed))\n","\n","        # Evaluate with fresh clusters.\n","        acc_list_new = []  \n","        res_list_new = []                 \n","        logger.info('Start computing centroids.')\n","        if args.repeats > 0:\n","            for _ in range(args.repeats):\n","                t1 = t.time()\n","                centroids1, kmloss1 = run_mini_batch_kmeans(args, logger, trainloader, model, view=-1)\n","                logger.info('-Centroids ready. [Loss: {:.5f}/ Time: {}]\\n'.format(kmloss1, get_datetime(int(t.time())-int(t1))))\n","                \n","                classifier1 = initialize_classifier(args)\n","                classifier1.module.weight.data = centroids1.unsqueeze(-1).unsqueeze(-1)\n","                freeze_all(classifier1)\n","                \n","                acc_new, res_new = evaluate(args, logger, testloader, classifier1, model)\n","                acc_list_new.append(acc_new)\n","                res_list_new.append(res_new)\n","        else:\n","            acc_new, res_new = evaluate(args, logger, testloader, classifier1, model)\n","            acc_list_new.append(acc_new)\n","            res_list_new.append(res_new)\n","\n","        logger.info('Average overall pixel accuracy [NEW] : {:.3f} +/- {:.3f}.'.format(np.mean(acc_list_new), np.std(acc_list_new)))\n","        logger.info('Average mIoU [NEW] : {:.3f} +/- {:.3f}. '.format(np.mean([res['mean_iou'] for res in res_list_new]), \n","                                                                    np.std([res['mean_iou'] for res in res_list_new])))\n","        logger.info('Experiment done. [{}]\\n'.format(get_datetime(int(t.time())-int(t_start))))"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2022-08-20T11:46:49.389242Z","iopub.status.busy":"2022-08-20T11:46:49.388800Z","iopub.status.idle":"2022-08-20T11:46:49.402937Z","shell.execute_reply":"2022-08-20T11:46:49.401914Z","shell.execute_reply.started":"2022-08-20T11:46:49.389209Z"},"trusted":true},"outputs":[],"source":["class Arguments:\n","    def __init__(self, \n","                data_root='../../Data/coco',\n","                save_root='results',\n","                restart_path='',\n","                seed=1,\n","                num_workers=4,\n","                restart=True,\n","                num_epoch=10,\n","                repeats=1,\n","                arch='resnet18',\n","                pretrain=True,\n","                res=320,\n","                res1=320,\n","                res2=640,\n","                batch_size_cluster=64,\n","                batch_size_train=64,\n","                batch_size_test=64,\n","                lr=1e-4,\n","                weight_decay=0,\n","                momentum=0.9,\n","                optim_type='Adam',\n","                num_init_batches=30,\n","                num_batches=30,\n","                kmeans_n_iter=30,\n","                in_dim=128,\n","                X=80,\n","                metric_train='cosine',\n","                metric_test='cosine',\n","                K_train=27,\n","                K_test=27,\n","                no_balance=False,\n","                mse=False,\n","                augment=False,\n","                equiv=False,\n","                min_scale=0.5,\n","                stuff=True,\n","                thing=True,\n","                jitter=False,\n","                grey=False,\n","                blur=False,\n","                h_flip=False,\n","                v_flip=False,\n","                random_crop=False,\n","                val_type='train',\n","                version=7,\n","                fullcoco=False,\n","                eval_only=False,\n","                eval_path='results',\n","                save_model_path='K_train',\n","                save_eval_path='K_test',\n","                cityscapes=False\n","                ):\n","\n","        self.data_root=data_root\n","        self.save_root=save_root\n","        self.restart_path=restart_path\n","        self.seed=seed\n","        self.num_workers=num_workers\n","        self.restart=restart\n","        self.num_epoch=num_epoch\n","        self.repeats=repeats\n","        self.arch=arch\n","        self.pretrain=pretrain\n","        self.res=res\n","        self.res1=res1\n","        self.res2=res2\n","        self.batch_size_cluster=batch_size_cluster\n","        self.batch_size_train=batch_size_train\n","        self.batch_size_test=batch_size_test\n","        self.lr=lr\n","        self.weight_decay=weight_decay\n","        self.momentum=momentum\n","        self.optim_type=optim_type\n","        self.num_init_batches=num_init_batches\n","        self.num_batches=num_batches\n","        self.kmeans_n_iter=kmeans_n_iter\n","        self.in_dim=in_dim\n","        self.X=X\n","        self.metric_train=metric_train\n","        self.metric_test=metric_test\n","        self.K_train=K_train\n","        self.K_test=K_test\n","        self.no_balance=no_balance\n","        self.mse=mse\n","        self.augment=augment\n","        self.equiv=equiv\n","        self.min_scale=min_scale\n","        self.stuff=stuff\n","        self.thing=thing\n","        self.jitter=jitter\n","        self.grey=grey\n","        self.blur=blur\n","        self.h_flip=h_flip\n","        self.v_flip=v_flip\n","        self.random_crop=random_crop\n","        self.val_type=val_type\n","        self.cityscapes = cityscapes\n","        self.version=version\n","        self.fullcoco=fullcoco\n","        self.eval_only=eval_only\n","        self.eval_path=eval_path\n","        self.save_eval_path = save_eval_path\n","        self.save_model_path = save_model_path\n","\n","\n","args = Arguments()"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2022-08-20T11:47:57.153791Z","iopub.status.busy":"2022-08-20T11:47:57.153291Z","iopub.status.idle":"2022-08-20T11:47:57.172376Z","shell.execute_reply":"2022-08-20T11:47:57.171089Z","shell.execute_reply.started":"2022-08-20T11:47:57.153756Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["<__main__.Arguments object at 0x7f34ef453310>\n","<__main__.Arguments object at 0x7f34ef453310>\n","Adam optimizer is used.\n","Adam optimizer is used.\n","No checkpoint found at [K_train/checkpoint.pth.tar].\n","Start from beginning...\n","\n","No checkpoint found at [K_train/checkpoint.pth.tar].\n","Start from beginning...\n","\n","====== METRIC TEST : cosine ======\n","\n","====== METRIC TEST : cosine ======\n","\n","Batch input size   : [64, 3, 320, 320]\n","Batch input size   : [64, 3, 320, 320]\n","Batch label size   : [64, 320, 320]\n","Batch label size   : [64, 320, 320]\n","Batch feature size : [64, 128, 80, 80]\n","\n","Batch feature size : [64, 128, 80, 80]\n","\n","0/34\n","0/34\n","20/34\n","20/34\n","ACC  - All: 15.9623\n","ACC  - All: 15.9623\n","mIOU - All: 3.2437\n","mIOU - All: 3.2437\n","ACC  - Thing: 37.9450\n","ACC  - Thing: 37.9450\n","mIOU - Thing: 6.7283\n","mIOU - Thing: 6.7283\n","ACC  - Stuff: 24.7410\n","ACC  - Stuff: 24.7410\n","mIOU - Stuff: 6.9523\n","mIOU - Stuff: 6.9523\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m/home/nova/Lab_individual_works/Daniyal/thesis/Code/PiCIE/picie.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223139322e3136382e3130302e313734222c2275736572223a226e6f7661227d/home/nova/Lab_individual_works/Daniyal/thesis/Code/PiCIE/picie.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m\u001b[39m==\u001b[39m\u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223139322e3136382e3130302e313734222c2275736572223a226e6f7661227d/home/nova/Lab_individual_works/Daniyal/thesis/Code/PiCIE/picie.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     logger \u001b[39m=\u001b[39m set_logger(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(args\u001b[39m.\u001b[39msave_eval_path, \u001b[39m'\u001b[39m\u001b[39mtrain.log\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223139322e3136382e3130302e313734222c2275736572223a226e6f7661227d/home/nova/Lab_individual_works/Daniyal/thesis/Code/PiCIE/picie.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     main(args, logger)\n","\u001b[1;32m/home/nova/Lab_individual_works/Daniyal/thesis/Code/PiCIE/picie.ipynb Cell 12\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args, logger)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223139322e3136382e3130302e313734222c2275736572223a226e6f7661227d/home/nova/Lab_individual_works/Daniyal/thesis/Code/PiCIE/picie.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(args\u001b[39m.\u001b[39mstart_epoch, args\u001b[39m.\u001b[39mnum_epoch):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223139322e3136382e3130302e313734222c2275736572223a226e6f7661227d/home/nova/Lab_individual_works/Daniyal/thesis/Code/PiCIE/picie.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m     \u001b[39m# Assign probs. \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223139322e3136382e3130302e313734222c2275736572223a226e6f7661227d/home/nova/Lab_individual_works/Daniyal/thesis/Code/PiCIE/picie.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m     trainloader\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39mmode \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcompute\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223139322e3136382e3130302e313734222c2275736572223a226e6f7661227d/home/nova/Lab_individual_works/Daniyal/thesis/Code/PiCIE/picie.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m     trainloader\u001b[39m.\u001b[39;49mdataset\u001b[39m.\u001b[39;49mreshuffle()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223139322e3136382e3130302e313734222c2275736572223a226e6f7661227d/home/nova/Lab_individual_works/Daniyal/thesis/Code/PiCIE/picie.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m     \u001b[39m# Adjust lr if needed. \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223139322e3136382e3130302e313734222c2275736572223a226e6f7661227d/home/nova/Lab_individual_works/Daniyal/thesis/Code/PiCIE/picie.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m     \u001b[39m# adjust_learning_rate(optimizer, epoch, args)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223139322e3136382e3130302e313734222c2275736572223a226e6f7661227d/home/nova/Lab_individual_works/Daniyal/thesis/Code/PiCIE/picie.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m============================= [Epoch \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m] =============================\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(epoch))\n","File \u001b[0;32m~/Lab_individual_works/Daniyal/thesis/Code/PiCIE/data/coco_train_dataset.py:78\u001b[0m, in \u001b[0;36mTrainCOCO.reshuffle\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshuffled_indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marange(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimdb))\n\u001b[1;32m     77\u001b[0m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mshuffle(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshuffled_indices)\n\u001b[0;32m---> 78\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minit_transforms()\n","File \u001b[0;32m~/Lab_individual_works/Daniyal/thesis/Code/PiCIE/data/coco_train_dataset.py:163\u001b[0m, in \u001b[0;36mTrainCOCO.init_transforms\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrandom_horizontal_flip \u001b[39m=\u001b[39m RandomHorizontalTensorFlip(N\u001b[39m=\u001b[39mN)\n\u001b[1;32m    162\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrandom_vertical_flip   \u001b[39m=\u001b[39m RandomVerticalFlip(N\u001b[39m=\u001b[39mN)\n\u001b[0;32m--> 163\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrandom_resized_crop    \u001b[39m=\u001b[39m RandomResizedCrop(N\u001b[39m=\u001b[39;49mN, res\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mres1, scale\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale)\n\u001b[1;32m    165\u001b[0m \u001b[39m# Tensor transform. \u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform_tensor \u001b[39m=\u001b[39m TensorTransform()\n","File \u001b[0;32m~/Lab_individual_works/Daniyal/thesis/Code/PiCIE/data/custom_transforms.py:211\u001b[0m, in \u001b[0;36mRandomResizedCrop.__init__\u001b[0;34m(self, N, res, scale)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mres    \u001b[39m=\u001b[39m res\n\u001b[1;32m    210\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale  \u001b[39m=\u001b[39m scale \n\u001b[0;32m--> 211\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrscale \u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39muniform(\u001b[39m*\u001b[39mscale) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(N)]\n\u001b[1;32m    212\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrcrop  \u001b[39m=\u001b[39m [(np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39muniform(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m), np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39muniform(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(N)]\n","File \u001b[0;32m~/Lab_individual_works/Daniyal/thesis/Code/PiCIE/data/custom_transforms.py:211\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mres    \u001b[39m=\u001b[39m res\n\u001b[1;32m    210\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale  \u001b[39m=\u001b[39m scale \n\u001b[0;32m--> 211\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrscale \u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49muniform(\u001b[39m*\u001b[39;49mscale) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(N)]\n\u001b[1;32m    212\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrcrop  \u001b[39m=\u001b[39m [(np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39muniform(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m), np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39muniform(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(N)]\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["if __name__=='__main__':\n","    logger = set_logger(os.path.join(args.save_eval_path, 'train.log'))\n","    main(args, logger)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.8.13 ('picie')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"vscode":{"interpreter":{"hash":"8e4808a2716005a07288033b82400c83a06994f7fabcbc90ba144df6892cd854"}}},"nbformat":4,"nbformat_minor":4}
